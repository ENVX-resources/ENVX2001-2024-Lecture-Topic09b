---
title: "Regression: predictive modelling -- Part 2"
subtitle: "ENVX2001 - Applied Statistical Methods"
author:
  - name: Januar Harianto
    affiliations: The University of Sydney
date: last-modified
self-contained: true
execute:
  freeze: auto
  cache: false
# NOTE: please check _quarto.yml file for more options
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE,
    cache = TRUE
)
library(tidyverse)
ggplot2::theme_set(cowplot::theme_half_open())
# ggplot2::theme_set(ggplot2::theme_minimal())
```




# Recap

## Model validation workflow

1. **Split** the data into training and test sets (not necessary for cross-validation techniques)

. . .
  
2. **Develop model(s)** on the training set (includes variable selection).

. . .

3. **Validate** the model(s) by comparing to the test set:
   - **RMSE** - root mean squared error: a measure of accuracy.
     - *The lower, the better*
   - **ME** - mean error: a measure of bias.
     - *The closer to 0, the better*
   - **CCC** - concordance correlation coefficient: a measure of agreement/precision.
     - *The closer to 1, the better*

# Example: Loyn dataset

## About

Data on the relationship between bird abundance (bird ha^-1^) and the characteristics of forest patches at 56 locations in SE Victoria.  

The predictor variables are:

- `ALT` Altitude (m) 
- `YR.ISOL` Year when the patch was isolated (years) 
-	`GRAZE` Grazing (coded 1-5 which is light to heavy) 
-	`AREA` Patch area (ha) 
-	`DIST` Distance to nearest patch (km) 
-	`LDIST` Distance to largest patch (km)  

```{r}
library(readxl)
loyn <- read_xlsx("assets/mlr.xlsx", "Loyn")
```


## Dataset splitting

We will split the data into training and test sets.

:::{.callout-note}
Sometimes, the training and tests sets are also called the *calibration* and *validation* sets, respectively.
:::

As the dataset is quite small, we will use a 80:20 split.

```{r}
set.seed(100)
indexes <- sample(1:nrow(loyn), size = 0.2 * nrow(loyn))
loyn_train <- loyn[-indexes, ] # use this to create model
loyn_test <- loyn[indexes, ] # use this to validate model
```

## Checking the split

Have a quick `glimpse()` of the data to see if the split worked. If your data does not look the same as below, you may have forgotten to set the seed.

```{r}
#| message: false
library(tidyverse)
glimpse(loyn_train)
glimpse(loyn_test)
```

# Model development
From now on, we will work with the **training set only**.

## Explore

- The next step is to visualise the data.
- Expore relationships between the predictors and the response. 
  - Histograms
  - Correlation plots
  - Boxplots

In this lecture we will just look at histograms.

## Histograms

```{r}
library(Hmisc)
hist(loyn_train, nclass = 20)
```

- Looks like `AREA` `LDIST` and `DIST` are skewed -- we will transform them so that they are more normally distributed.

## Transforming predictors

 We will use `log10()` to transform the predictors. The `mutate()` function from the `dplyr` package is useful for this as it can create new columns in the data frame with the transformed values.

```{r}
loyn_train <- loyn_train %>%
    mutate(
        AREA_L10 = log10(AREA),
        LDIST_L10 = log10(LDIST),
        DIST_L10 = log10(DIST)
    )
```

Then, remove the untransformed variables from the dataset. Here we can use the `select()` function from the `dplyr` package to "delselect" columns by using the `-` sign.

```{r}
loyn_train <- loyn_train %>%
    select(-AREA, -LDIST, -DIST)
glimpse(loyn)
```

## Final inspection

View the histograms again to check that the transformation worked.

```{r}
hist(loyn_train, nclass = 20)
```

## Full model

We start with a full model that includes all the predictors.

```{r}
full_fit <- lm(ABUND ~ ., data = loyn_train)
summary(full_fit)
```

## Assumptions - Round 1

As usual, we should check the assumptions of the model. We will use the `check_model()` function from the `perfomance` package as it also has VIF plots, which we will use.


```{r}
library(performance)
check_model(full_fit)
```

- Overall, all the assumptions are met. VIFs are all < 10, so no multicollinearity.

# Variable selection

## Backwards stepwise selection

Use the `step()` function perform backwards stepwise selection. This function uses AIC to select the best model. 

Depending on the dataset splitting, the best model may be different each time we randomly sample the data. In this case we should all have the same results as we set the seed.


```{r}
step_fit <- step(full_fit, direction = "backward")
```

- If we compare to the full model, the adjusted r-squared is slightly higher, and the AIC is lower.

## The selected model

```{r}
summary(step_fit)
```

## Assumptions - Round 2

```{r}
check_model(step_fit)
```

# Model validation
It looks like the model is good, so let's bring in the test set to see how it performs!

## Assess prediction quality

- RMSE: root mean squared error

$$ RMSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$

- ME: mean error - also commonly called **bias**

$$ ME = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i) $$


- CCC: Lin's concordance correlation coefficient
- We will also look at correlations (so as to compare with CCC)
- All of these have been implemented as functions in various packages, so we can also use those.

## Performing the tests

```{r}
library(caret)
library(epiR)
```

The following functions are available:

- `RMSE()` from the `caret` package: `RMSE(y, y_hat)`
- `epi.ccc()` from the `epiR` package: `epi.ccc(y, y_hat)`
- `cor()` from base R: `cor(y, y_hat)`

We need to use calculations for ME: `mean(y - y_hat)`

Recall:

- full model: `full_fit`
- stepwise model: `step_fit`
- test data: `loyn_test`

## Prepare the test data

Since the test data has not been transformed, we need to do that first.

```{r}
loyn_test <- loyn_test %>%
    mutate(
        AREA_L10 = log10(AREA),
        LDIST_L10 = log10(LDIST),
        DIST_L10 = log10(DIST)
    ) %>%
    select(-AREA, -LDIST, -DIST)
```

## Perform tests (1)

#### RMSE - lower is better

```{r}
RMSE(loyn_train$ABUND, predict(step_fit))
RMSE(loyn_test$ABUND, predict(step_fit, newdata = loyn_test))
```

*RMSE is larger in the test set because the model was not trained on that data. Looks like the model is overfitting.*


#### ME - lower is better

```{r}
mean(loyn_train$ABUND - predict(step_fit))
mean(loyn_test$ABUND - predict(step_fit, newdata = loyn_test))
```

*ME is smaller in the training set because of the larger sample size.* 


## Perform tests (2)

Correlation - higher is better

```{r}
cor(loyn_train$ABUND, predict(step_fit))
cor(loyn_test$ABUND, predict(step_fit, newdata = loyn_test))
```

*Correlation is similar in both sets.*

CCC - higher is better

```{r}
epi.ccc(loyn_train$ABUND, predict(step_fit))$rho.c
epi.ccc(loyn_test$ABUND, predict(step_fit, newdata = loyn_test))$rho.c
```

*CCC is higher in training set, indicating that the model is overfitting.*

## Conclusions

```{r}
#| code-fold: true
# put all data into a tible and kable it
tibble(
    Dataset = c("Training", "Test"),
    RMSE = c(
        RMSE(loyn_train$ABUND, predict(step_fit)),
        RMSE(loyn_test$ABUND, predict(step_fit, newdata = loyn_test))
    ),
    ME = c(
        mean(loyn_train$ABUND - predict(step_fit)),
        mean(loyn_test$ABUND - predict(step_fit, newdata = loyn_test))
    ),
    COR = c(
        cor(loyn_train$ABUND, predict(step_fit)),
        cor(loyn_test$ABUND, predict(step_fit, newdata = loyn_test))
    ),
    CCC = c(
        epi.ccc(loyn_train$ABUND, predict(step_fit))$rho.c[[1]],
        epi.ccc(loyn_test$ABUND, predict(step_fit, newdata = loyn_test))$rho.c[[1]]
    )
) %>%
    knitr::kable()
```

- The model is overfitting, but not too badly (differences are small enough between training and test sets)
- Large difference in ME is expected, as the training set is much larger than the test set (so the mean is more stable)
- Overfitting indicates that model is still too complex: we should try to simplify it further.
- This could be due to the small sample size, or the fact that we have too many predictors, or the `set.seed()` function causing the model(s) to be different each time.

# Thanks!

**Questions? Comments?**

Slides made with [Quarto](https://quarto.org)
